#!/bin/sh -e

##########################################################################
#   Script description:
#       Run multiple ad2vcf.sh jobs together
#       
#   History:
#   Date        Name        Modification
#   2020-07-28              Begin
##########################################################################

usage()
{
    printf "Usage: $0 sample-file\n"
    exit 1
}


##########################################################################
#   Main
##########################################################################

if [ $# != 1 ]; then
    usage
fi
sample_file=$1
# 6 jobs seems ideal for a 16-vCPU c5a-4xlarge VM
# Running 10 of these, so 60 ad2vcf jobs in parallel.
# These are 16 hyperthreads, 8 full CPUs, so 8 2-core jobs too much and
# actually reduces jobs/hr slightly vs 6 jobs/VM.
# I lean against using higher-count VMs since the VM's network interface
# could become a bottleneck.  Better to spread the I/O load out to some
# extent so each VM can breathe easy reading CRAMs.
job_count=6
vcf_dir=VCFs-$(basename $sample_file)
mkdir -p Logs

prefix=$(basename $sample_file)-
split -d -n l/$job_count $sample_file $prefix
for file in ${prefix}??; do
    echo $file
    if [ -e Logs/$file.out ]; then
	printf "Logs/$file.out already exists.  Move or remove and try again.\n"
    else
	nohup ./AWS-scripts/ad2vcf.sh $file $vcf_dir > Logs/$file.out 2> Logs/$file.err &
	printf "Sample file = %s  Job count = %s  VCF-dir = %s\n" \
	    $sample_file $job_count $vcf_dir > Logs/job-info
    fi
done
top
